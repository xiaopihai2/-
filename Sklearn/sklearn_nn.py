#神经网络分类(多层感知器)；神经网络回归

#神经网络分类：
    #MLPClassifier 只支持交叉熵损失函数，通过运行 predict_proba 方法进行概率估计。核心是BP算法
import numpy as np
import matplotlib.pyplot as plt
from sklearn.neural_network import MLPClassifier
from sklearn.preprocessing import StandardScaler

data = [
    [-0.017612, 14.053064, 0],[-1.395634, 4.662541, 1],[-0.752157, 6.53862, 0],[-1.322371, 7.152853, 0],[0.423363, 11.054677, 0],
    [0.406704, 7.067335, 1],[0.667394, 12.741452, 0],[-2.46015, 6.866805, 1],[0.569411, 9.548755, 0],[-0.026632, 10.427743, 0],
    [0.850433, 6.920334, 1],[1.347183, 13.1755, 0],[1.176813, 3.16702, 1],[-1.781871, 9.097953, 0],[-0.566606, 5.749003, 1],
    [0.931635, 1.589505, 1],[-0.024205, 6.151823, 1],[-0.036453, 2.690988, 1],[-0.196949, 0.444165, 1],[1.014459, 5.754399, 1],
    [1.985298, 3.230619, 1],[-1.693453, -0.55754, 1],[-0.576525, 11.778922, 0],[-0.346811, -1.67873, 1],[-2.124484, 2.672471, 1],
    [1.217916, 9.597015, 0],[-0.733928, 9.098687, 0],[1.416614, 9.619232, 0],[1.38861, 9.341997, 0],[0.317029, 14.739025, 0]
]
dataMat = np.array(data)
X = dataMat[:, 0:2]
y = dataMat[:, 2]
scaler = StandardScaler()
X = scaler.fit_transform(X)
"""
    alpha:正则化项的参数，默认为l2正则化项
    solver:MLP的求解方法：L-BFGS 在小数据上表现较好，Adam 较为鲁棒，SGD在参数调整较优时会有最佳表现（分类效果与迭代次数）；SGD标识随机梯度下降。
    hidden_layer_sizes:隐藏层的每层的神经元的个数
    下面：hidden层2层,第一层5个神经元，第二层2个神经元)，2层隐藏层，也就有3层神经网络
"""
MLP = MLPClassifier(alpha=1e-5, solver='lbfgs', hidden_layer_sizes=(5, 2), random_state=1)
MLP.fit(X, y)
print("每层神经元的系数矩阵维度：", [cofe.shape for cofe in MLP.coefs_])
y_predict = MLP.predict([[0.317029, 14.739025]])
print("预测结果：", y_predict)
y_p = MLP.predict_proba([[0.317029, 14.739025]])
print("预测的概率：", y_p)
for i, clf in enumerate(MLP.coefs_):
    print("第%d层神经网络:", i+1)
    print("该层的系数矩阵维度;", clf.shape)
    print("该层的系数矩阵：", clf)

#绘制分割区域
x_min, x_max = X[:,0].min() -1, X[:,0].max() +1
y_min, y_max = X[:,1].min()-1, X[:,1].max()+1
xx1, xx2 = np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01))
print(xx1, xx1.ravel(), np.c_[xx1.ravel(), xx2.ravel()],sep='\n')
Z = MLP.predict(np.c_[xx1.ravel(), xx2.ravel()]) # 先形成待测样本的形式，在通过模型进行预测。
Z = Z.reshape(xx1.shape) # 将输出结果转换为和网格的矩阵形式，以便绘图
#绘制网格区域图
plt.pcolormesh(xx1, xx2, Z, cmap=plt.cm.Paired)
plt.scatter(X[:,0], X[:,1], c = y)
plt.show()

#神经网络回归：平方误差作为损失函数
from sklearn.neural_network import MLPRegressor
data = [
         [ -0.017612,14.053064,14.035452],[ -1.395634, 4.662541, 3.266907],[ -0.752157, 6.53862,5.786463],[ -1.322371, 7.152853, 5.830482],
         [0.423363,11.054677,11.47804 ],[0.406704, 7.067335, 7.474039],[0.667394,12.741452,13.408846],[ -2.46015,6.866805, 4.406655],
         [0.569411, 9.548755,10.118166],[ -0.026632,10.427743,10.401111],[0.850433, 6.920334, 7.770767],[1.347183,13.1755,14.522683],
         [1.176813, 3.16702,4.343833],[ -1.781871, 9.097953, 7.316082],[ -0.566606, 5.749003, 5.182397],[0.931635, 1.589505, 2.52114 ],
         [ -0.024205, 6.151823, 6.127618],[ -0.036453, 2.690988, 2.654535],[ -0.196949, 0.444165, 0.247216],[1.014459, 5.754399, 6.768858],
         [1.985298, 3.230619, 5.215917],[ -1.693453,-0.55754, -2.250993],[ -0.576525,11.778922,11.202397],[ -0.346811,-1.67873, -2.025541],
         [ -2.124484, 2.672471, 0.547987],[1.217916, 9.597015,10.814931],[ -0.733928, 9.098687, 8.364759],[1.416614, 9.619232,11.035846],
         [1.38861,9.341997,10.730607],[0.317029,14.739025,15.056054]
]

dataMat = np.mat(data)
X = dataMat[:, 0:2]
y = dataMat[:, 2]
#归一化
X = scaler.transform(X)
model = MLPRegressor(alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=4, solver='lbfgs')
model.fit(X, y)

y_predict = model.predict([[0.317029, 14.739025]])
print("预测结果：", y_predict)
for i, clf in enumerate(model.coefs_):
    print("第%d层神经网络:", i+1)
    print("该层的系数矩阵维度;", clf.shape)
    print("该层的系数矩阵：", clf)
